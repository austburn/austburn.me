<p>On the <a class="content-link" href="https://mycloud.rackspace.com">Rackspace Cloud Control Panel</a> team, we use the <a class="content-link" href="https://www.elastic.co/webinars/introduction-elk-stack">ELK Stack</a>
to collect and visualize our logs. You can find more information online, but out of convenience:</p>
<ul>
<li><strong>Elasticsearch</strong> - Used to index logs and make them searchable.</li>
<li><strong>Logstash</strong> - Takes any log and allows you to create a filter to pull out the information you are interested in and format it for
Elasticsearch indexing.</li>
<li><strong>Kibana</strong> - Pretty web application that leverages the Elasticsearch api.</li>
</ul>
<p>Being on-call occasionally, I utilize an IRC bouncer in order to keep track of logs so that I don't have to. Just because my bouncer keeps track of
my logs doesn't imply that they're easy to reference. As a result, I decided to set up an ELK stack for my IRC logs. Probably a little overkill, but
it presents itself as a useful exercise that others might be able to relate to.</p>
<p>This post will be part of a 2 (or 3) part series. First, we'll set up Logstash to parse our IRC logs.</p>
<p>Let's start with some details:</p>
<ul>
<li>I have a lot of logs.</li>
<li>Inside the log files, there are two formats:</li>
<ul>
  <li>[hh:mm:ss] &lt;user&gt; &lt;log&gt;</li>
  <li>[hh:mm:ss] &lt;log&gt;</li>
</ul>
<li>Logs are titled: user_network_channel_yyyymmdd.log</li>
</ul>
<p>I would put how to get started with Logstash, but my teammate Matt Green put together a
<a class="content-link" href="https://medium.com/@thematthewgreen/getting-started-with-logstash-62f0a573685b">thorough guide</a>. The guide will take you
through the basics of Logstash, getting it set up, and how to write Logstash configurations.</p>
<p>With that, let's get started. When writing a new filter, I usually just set up a basic config that utilizes <code>stdin</code>/<code>stdout</code> and
as per Matt's suggestion, use <a class="content-link" href="https://grokdebug.herokuapp.com/">Grok Debugger</a> to create a basic filter:</p>
  <pre><code>input {
  stdin { }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\&gt; &#37;{GREEDYDATA:msg}" ]
    }
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
<p>This Logstash configuration enables us to filter our first type of log, a basic log with a timestamp, username, and a message. We'll use the <code>GREEDYDATA</code>
pattern because both usernames and messages can contain symbols that the <code>WORD</code> pattern does not include.</p>
<p class="tip"><strong>TIP:</strong> You can find a list of Logstash grok patterns
  <a class="content-link" href="https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns">here</a>.
</p>
<p>Let's take a look at what our config will do:</p>
  <pre><code class="custom-bash">$ bin/logstash  -f grok_config
Logstash startup completed
[07:25:53] &lt;austburn&gt; ping
{
     "message" => "[07:25:53] &lt;austburn&gt; ping",
    "@version" => "1",
  "@timestamp" => "2015-10-14T19:34:26.866Z",
        "time" => "07:25:53",
    "username" => "austburn",
         "msg" => "ping"
}</code></pre>
<p>Awesome, our config worked as expected.</p>
<p>What if we feed it a system log?</p>
  <pre><code class="custom-bash">$ bin/logstash  -f grok_config
Logstash startup completed
[07:25:53] austin is now known as austburn
{
     "message" => "[07:25:53] austin is now known as austburn",
    "@version" => "1",
  "@timestamp" => "2015-10-19T17:41:58.789Z",
        "tags" => [
      [0] "_grokparsefailure"
  ]
}</code></pre>
<p><code>_grokparsefailure</code> is not what we wanted. Since we were looking for a username enclosed in angle brackets, grok didn't appreciate our log
not adhering to that style.</p>
<p>Logstash supports multiple grok patterns. Be careful as Logstash attempts to parse logs in the order of the patterns. I made this mistake by putting a more
lenient pattern first and it absorbed all my logs. As a rule of thumb, you'd want to have your least generic filter take precedence.</p>
  <pre><code>input {
  stdin { }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\&gt; &#37;{GREEDYDATA:msg}", "\[&#37;{TIME:time}\] &#37;{GREEDYDATA:msg}" ]
    }
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
  <pre><code class="custom-bash">$ bin/logstash  -f grok_config
Logstash startup completed
[07:25:53] austin is now known as austburn
{
     "message" => "[07:25:53] austin is now known as austburn",
    "@version" => "1",
  "@timestamp" => "2015-10-19T17:54:59.859Z",
        "time" => "07:25:53",
         "msg" => "austin is now known as austburn"
}
[07:25:53] &lt;austburn&gt; ping
{
     "message" => "[07:25:57] &lt;austburn&gt; ping",
    "@version" => "1",
  "@timestamp" => "2015-10-19T17:56:25.761Z",
        "time" => "07:25:57",
    "username" => "austburn",
         "msg" => "ping"
}</code></pre>
<p>Now our filter accepts and parses both types of logs. In reality, I don't really care if austin changed his IRC nickname at all. I don't need to index these
logs or look at them ever again, so we're actually going to ignore them.</p>
<p>Normally, you probably would not want to drop all logs that produce <code>_grokparsefailure</code>'s, but being that IRC logs are pretty straightforward, this
is OK.</p>
  <pre><code>input {
  stdin { }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\&gt; &#37;{GREEDYDATA:msg}" ]
    }
  }
  if "_grokparsefailure" in [tags] {
    drop { }
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
<p>While using <code>stdin</code> is great for debugging and general testing, it's not how we'll ultimately consume the logs. For right now, I plan to read
logs from disk. There is a <code>file</code> plugin for the input that we will use.</p>
  <pre><code>input {
  file {
    path => ["/data/log/*"]
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\&lt; &#37;{GREEDYDATA:msg}" ]
    }
  }
  if "_grokparsefailure" in [tags] {
    drop {}
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
<p>Let's talk through this config:</p>
<ul>
  <li><strong>path</strong>: Where the logs we want are located on disk</li>
  <li><strong>start_position</strong>: Tell where Logstash to begin parsing files. By default, this is set to <code>"end"</code> because Logstash expects traditional
  log behavior where new logs are appended to the file.</li>
  <li><strong>sincedb_path</strong>: Really only necessary for the ability to reprocess logs. Logstash keeps track of how much of a file it has parsed so
    that it does not reprocess logs. By setting this to <code>/dev/null</code>, Logstash effectively has no memory and will reprocess logs each run. We will
  want to remove this when we begin to actually consume logs.</li>
</ul>
<p>As I mentioned before, ZNC stores some valuable data in the name of the log file including the user, network, channel, and date. If you've noticed, our
timestamps are not linked to the time the log was written. This information will be important for indexing logs and searching for logs base on time.</p>
<p>Because we're now consuming logs from file, we have access to the filename via the path attribute. We can add a second grok pattern and add this information
to the log.</p>
  <pre><code>input {
  file {
    path => ["/data/log/*"]
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\&lt; &#37;{GREEDYDATA:msg}" ]
    }
    match => {
      "path" => [ "&#37;{UNIXPATH:location}\/&#37;{GREEDYDATA:znc_user}_&#37;{GREEDYDATA:network}_#&#37;{GREEDYDATA:channel}_&#37;{YEAR:year}&#37;{MONTHNUM:month}&#37;{MONTHDAY:day}" ]
    }
    break_on_match => false
  }
  if "_grokparsefailure" in [tags] {
    drop {}
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
<p>Let's see what this looks like:</p>
  <pre><code class="custom-bash">$ bin/logstash  -f grok_config
Logstash startup completed
{
     "message" => "[19:43:13] &lt;austburn&gt; hey",
    "@version" => "1",
  "@timestamp" => "2015-10-19T19:57:31.748Z",
        "path" => "/data/log/austburn_freenode_#austin_channel_20150615.log",
        "time" => "19:43:13",
    "username" => "austburn",
         "msg" => "hey",
    "location" => "/data/log",
    "znc_user" => "austburn",
     "network" => "freenode",
     "channel" => "austin_channel",
        "year" => "2015",
       "month" => "06",
         "day" => "15"
}</code></pre>
<p>We now have all the information to craft the timestamp based on the filename and time in the log message itself.</p>
<p>To achieve this, we are going to have to construct a timestamp from the information we have and apply the <code>date</code> filter in
order to manipulate the <code>@timestamp</code> field.</p>
  <pre><code>input {
  file {
    path => ["/data/log/*"]
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  grok {
    match => {
      "message" => [ "\[&#37;{TIME:time}\] \&lt;&#37;{GREEDYDATA:username}\> &#37;{GREEDYDATA:msg}" ]
    }
    match => {
      "path" => [ "&#37;{UNIXPATH:location}\/&#37;{GREEDYDATA:znc_user}_&#37;{GREEDYDATA:network}_#&#37;{GREEDYDATA:channel}_&#37;{YEAR:year}&#37;{MONTHNUM:month}&#37;{MONTHDAY:day}" ]
    }
    break_on_match => false
  }
  if "_grokparsefailure" in [tags] {
    drop {}
  }
  mutate {
    add_field => { "message_timestamp" => "&#37;{year}-&#37;{month}-&#37;{day};&#37;{time}" }
  }
  date {
    match => [ "message_timestamp", "YYYY-MM-dd;HH:mm:ss" ]
    target => "@timestamp"
  }
  mutate {
    remove_field => [ "message_timestamp", "year", "month", "day", "time" ]
  }
}

output {
  stdout { codec => rubydebug }
}</code></pre>
<p>This config will:</p>
<ol>
  <li>Grab the time information from the IRC log as well as the filename.</li>
  <li>Create a <code>message_timestamp</code> field based on the information we collected.</li>
  <li>Use the date filter to parse the <code>message_timestamp</code> field and assign the value to the <code>@timestamp</code> field.</li>
  <li>Remove all the time related fields that we no longer need.</li>
</ol>
<p>Let's see what the final log looks like:</p>
  <pre><code class="custom-bash">$ bin/logstash  -f grok_config
Logstash startup completed
{
     "message" => "[19:43:13] &lt;austburn&gt; hey",
    "@version" => "1",
  "@timestamp" => "2015-06-15T19:43:13.000Z",
        "path" => "/data/log/austburn_freenode_#austin_channel_20150615.log",
    "username" => "austburn",
         "msg" => "hey",
    "location" => "/data/log",
    "znc_user" => "austburn",
     "network" => "freenode",
     "channel" => "austin_channel"
}</code></pre>
<p>Great, now our logs parsed and include all the information we need (or have access to for that matter) to start indexing.</p>
<p>That will conclude this part of the series. In conclusion we:</p>
<ul>
  <li>Set up Logstash.</li>
  <li>Built a basic Logstash config utilizing <code>stdin</code>.</li>
  <li>Filtered out IRC system logs.</li>
  <li>Created a Logstash filter for reading from disk.</li>
  <li>Gathered interesting information from the filename.</li>
  <li>Used the information we had to give the log an accurate timestamp.</li>
</ul>
<p>If you have any questions, feel free to ping me on <a class="content-link" href="https://twitter.com/austburn">Twitter</a>!</p>
